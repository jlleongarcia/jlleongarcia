Learn how to monitor LLMs workload like a Pro.

We are going to use the NetData, together with the Ollama project, and we are going to SelfHost it with Docker. Then, we will see how our infrastructure is doing when we are utilizing our local LLMs.

Feel free to deploy this at home, or at the cloud - same process applies.


ğŸ”—ğŸ”—ğŸ”— Stack to deploy NetData: https://fossengineer.com/selfhosting-server-monitoring-with-netdata-and-docker/

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

ğŸ’» Used During The Video
LLM used: https://ollama.ai/library/mistral
NetData Docker Image ğŸ‹: https://hub.docker.com/r/netdata/netdata


ğŸŒŸ Resources you might be Interested ğŸŒŸ

* https://fossengineer.com/tags/gen-ai/
* SelfHost NGINX to provide https to Ollama: https://fossengineer.com/selfhosting-nginx-proxy-manager-docker/

* LLama-Index (Pypi): https://pypi.org/project/llama-index/
* Ollama + LogSec: https://fossengineer.com/selfhosting-logseq/

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

ğŸ‘‰ What Software have I used during the video?

It is all Free Software that you can install in Windows/Mac/Linux:

Docker: https://docs.docker.com/get-docker/
Portainer to manage the containers with UI: https://fossengineer.com/selfhosting-portainer-docker/
VSCodium to create the Dockerfile: https://vscodium.com/
OBS Studio to record screen: https://jalcocert.github.io/Linux/docs/nix/fav-apps/#dev--content